{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "Requirement already satisfied: elementpath in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers==4.45.2 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dhlabadmin\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.45.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.45.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dhlabadmin\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhlabadmin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers==4.45.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.45.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.45.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.45.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.45.2) (2024.12.14)\n",
      "Collecting git+https://github.com/huggingface/transformers@muellerzr-more-models-sadface\n",
      "  Cloning https://github.com/huggingface/transformers (to revision muellerzr-more-models-sadface) to c:\\users\\dhlabadmin\\appdata\\local\\temp\\pip-req-build-y38i65r2\n",
      "  Resolved https://github.com/huggingface/transformers to commit f8a963c116e6df9fb44f48da6875c12392e6e787\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dhlabadmin\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.49.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.49.0.dev0)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.49.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dhlabadmin\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhlabadmin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers==4.49.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.49.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.49.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.49.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhlabadmin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.49.0.dev0) (2024.12.14)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.49.0.dev0-py3-none-any.whl size=10720286 sha256=f52019a311f03b9c7de02cb58aee8f86f46ed6863b49452d35bd376546830388\n",
      "  Stored in directory: C:\\Users\\dhlabadmin\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-dwrlwlee\\wheels\\0d\\3b\\21\\ca62faf95170d9088c43871a5d183504cd048515101fe487eb\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.2\n",
      "    Uninstalling transformers-4.45.2:\n",
      "      Successfully uninstalled transformers-4.45.2\n",
      "Successfully installed tokenizers-0.21.0 transformers-4.49.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\dhlabadmin\\AppData\\Local\\Temp\\pip-req-build-y38i65r2'\n",
      "  Running command git checkout -b muellerzr-more-models-sadface --track origin/muellerzr-more-models-sadface\n",
      "  branch 'muellerzr-more-models-sadface' set up to track 'origin/muellerzr-more-models-sadface'.\n",
      "  Switched to a new branch 'muellerzr-more-models-sadface'\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\dhlabadmin\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~-kenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install -q transformers\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q jiwer\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q -U accelerate\n",
    "\n",
    "!pip install -q matplotlib\n",
    "!pip install -q protobuf==3.20.1\n",
    "!pip install -q tensorboard\n",
    "!pip install elementpath\n",
    "!pip install scikit-learn\n",
    "!pip install numpy==1.26.4\n",
    "!pip install transformers==4.45.2\n",
    "!pip install git+https://github.com/huggingface/transformers@muellerzr-more-models-sadface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhlabadmin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\dhlabadmin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "block_plot = False\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "bold = f\"\\033[1m\"\n",
    "reset = f\"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "def get_text_regions(xml_file):\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        namespaces = {'ns': root.tag.split('}')[0].strip('{')}\n",
    "\n",
    "        regions = []\n",
    "\n",
    "        for region in root.findall(\".//ns:TextRegion\", namespaces):\n",
    "\n",
    "            for textline in region.findall(\".//ns:TextLine\", namespaces):\n",
    "                text_id = textline.get('id')\n",
    "                text_coords = textline.find(\".//ns:Coords\", namespaces).get('points')\n",
    "                unicode_elem = textline.find(\".//ns:TexEquiv/ns:Unicode\", namespaces)\n",
    "                if unicode_elem is not None and unicode_elem.text:\n",
    "                    unicode_text = unicode_elem.text\n",
    "                else:\n",
    "                    words = textline.findall(\".//ns:Word/ns:TextEquiv/ns:Unicode\", namespaces)\n",
    "                    unicode_text = \" \".join(w.text for w in words if w.text)\n",
    "\n",
    "                if text_coords:\n",
    "                    regions.append({'id': text_id, 'coords': text_coords, 'text': unicode_text})\n",
    "        return regions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed processing get_text_regions {xml_file}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def polygon_crop(image, points):\n",
    "    try:\n",
    "        if not points:\n",
    "            return None\n",
    "\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        points = np.array([list(map(int, p.split(','))) for p in points.split()], dtype=np.int32)\n",
    "        if len(points) < 3:\n",
    "            print(\"Not enough points\", points)\n",
    "        \n",
    "        cv2.fillPoly(mask, [points], 255)\n",
    "\n",
    "        res = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(points)\n",
    "        if w==0 or h==0:\n",
    "            print(\"w or h is zero\", w, h)\n",
    "        cropped_img = res[y:y+h, x:x+w]\n",
    "        return cropped_img\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed processing polygon_crop: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_dir, xml_dir, output_dir, output_csv):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = []\n",
    "\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        if not image_file.endswith((\".jpg\", \".png\", \".tif\", \"jpeg\")):\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "        if image is None:\n",
    "            print(\"image is None\", os.path.join(image_dir, image_file))\n",
    "\n",
    "        name = os.path.splitext(image_file)[0]\n",
    "        xml_path = os.path.join(xml_dir, name + \".xml\")\n",
    "\n",
    "        text_regions = get_text_regions(xml_path)\n",
    "\n",
    "        for i, region in enumerate(text_regions):\n",
    "            cropped_img = polygon_crop(image, region['coords'])\n",
    "\n",
    "            if cropped_img is None or cropped_img.size == 0:\n",
    "                print(f\"Failed processing {name}_{i:02}.png\")\n",
    "                continue\n",
    "\n",
    "            filename = f\"{name}_{i:02}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "            cv2.imwrite(save_path, cropped_img)\n",
    "\n",
    "            data.append({'filename': filename, 'text': region['text']})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = \"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048\"\n",
    "# image_file = \"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048/shchodennyk-0050.jpg\"\n",
    "# xml_dir = \"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048/page/shchodennyk-0050.xml\"\n",
    "# output_dir = os.path.join(image_dir, \"cropped_test\")\n",
    "# output_csv = os.path.join(image_dir, \"test.txt\")\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# data = []\n",
    "# image = cv2.imread(image_file)\n",
    "# if image is None:\n",
    "#     print(\"image is None\", image_file)\n",
    "\n",
    "# text_regions = get_text_regions(xml_dir)\n",
    "\n",
    "# for i, region in enumerate(text_regions):\n",
    "#     cropped_img = polygon_crop(image, region['coords'])\n",
    "\n",
    "#     if cropped_img is None or cropped_img.size == 0:\n",
    "#         print(f\"Failed processing test_{i:02}.png\")\n",
    "#         continue\n",
    "\n",
    "#     filename = f\"test_{i:02}.png\"\n",
    "#     save_path = os.path.join(output_dir, filename)\n",
    "#     cv2.imwrite(save_path, cropped_img)\n",
    "\n",
    "#     data.append({'filename': filename, 'text': region['text']})\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv(output_csv, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 6470048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed processing Akhtyrs'ka-0002_00.png\n",
      "Failed processing Akhtyrs'ka-0003_19.png\n",
      "Failed processing Akhtyrs'ka-0004_00.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0001_00.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0015_00.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0032_00.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0040_00.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0042_00.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0042_01.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0042_02.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0043_00.png\n",
      "Failed processing Letters_Shwedowa_11_2022-0043_01.png\n",
      "Failed processing Moroz_Dudyk-0004_00.png\n",
      "Failed processing Moroz_Dudyk-0004_17.png\n",
      "Failed processing Moroz_Dudyk-0007_00.png\n",
      "Failed processing Moroz_Dudyk-0007_04.png\n",
      "Failed processing Moroz_Dudyk-0007_15.png\n",
      "Failed processing Moroz_Dudyk-0009_00.png\n",
      "Failed processing Moroz_Dudyk-0009_05.png\n",
      "Failed processing Moroz_Dudyk-0009_06.png\n",
      "Failed processing Moroz_Dudyk-0009_10.png\n",
      "Failed processing Moroz_Dudyk-0010_00.png\n",
      "Failed processing Moroz_Dudyk-0019_00.png\n",
      "Failed processing Moroz_Dudyk-0020_00.png\n",
      "Failed processing shchodennyk-0043_00.png\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048/\"\n",
    "xml_dir = \"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048/page/\"\n",
    "output_dir = os.path.join(image_dir, \"cropped_6470048\")\n",
    "output_csv = os.path.join(image_dir, \"cropped_6470048.txt\")\n",
    "process_images(image_dir, xml_dir, output_dir, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048/cropped_6470048.txt\", delimiter=\",\", header=None, names=[\"file_name\", \"text\"], on_bad_lines=\"skip\")\n",
    "# dataset_prev = pd.read_csv(\"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048/tmp/cropped_6470048.txt\", delimiter=\",\", header=None, names=[\"file_name\", \"text\"], on_bad_lines=\"skip\")\n",
    "\n",
    "# old_text_dict = dict(zip(dataset_prev[\"file_name\"], dataset_prev[\"text\"]))\n",
    "# dataset[\"text\"] = dataset.apply(lambda row: old_text_dict.get(row[\"file_name\"], row[\"text\"]) if row[\"text\"] == \"\" else row[\"text\"], axis=1)\n",
    "# dataset.to_csv(\"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/6470048/df_crops_6470048.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 14484847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/14484847/\"\n",
    "xml_dir = \"C:/Users/dhlabadmin/Desktop/m-test/full-datasets/unpacked-datasets/14484847/page/\"\n",
    "output_dir = os.path.join(image_dir, \"cropped_14484847\")\n",
    "output_csv = os.path.join(image_dir, \"cropped_14484847.txt\")\n",
    "process_images(image_dir, xml_dir, output_dir, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(image_dir, \"cropped_14484847.txt\"), delimiter=\",\", header=None, names=[\"file_name\", \"text\"], on_bad_lines=\"skip\")\n",
    "\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ukrainka-0229_06.png</td>\n",
       "      <td>54. Славен у Бога Марисін по¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luk'anenko_last_pages-0003_09.png</td>\n",
       "      <td>по мамованно був зайтотий і я дітий.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shev_Kobzar-0048_43.png</td>\n",
       "      <td>Господа молити.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shchodennyk-0024_37.png</td>\n",
       "      <td>в нае почачась рознова щю Сжищів, в Якому я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Luk'anenko_last_pages-0007_26.png</td>\n",
       "      <td>зе паротя до Лохвиці, а там може спаю¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_name  \\\n",
       "0               Ukrainka-0229_06.png   \n",
       "1  Luk'anenko_last_pages-0003_09.png   \n",
       "2            Shev_Kobzar-0048_43.png   \n",
       "3            shchodennyk-0024_37.png   \n",
       "4  Luk'anenko_last_pages-0007_26.png   \n",
       "\n",
       "                                          text  \n",
       "0                54. Славен у Бога Марисін по¬  \n",
       "1         по мамованно був зайтотий і я дітий.  \n",
       "2                              Господа молити.  \n",
       "3  в нае почачась рознова щю Сжищів, в Якому я  \n",
       "4       зе паротя до Лохвиці, а там може спаю¬  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ukrainka-0126_04.png</td>\n",
       "      <td>на одну підводу скрині та перини,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luk'anenko_last_pages-0042_26.png</td>\n",
       "      <td>загубив мене, відбив від життя.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ukrainka-0080_16.png</td>\n",
       "      <td>-В Іванихи породіллі дитя калихала.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shchodennyk-0026_08.png</td>\n",
       "      <td>-Все розповів до найметшія дрібниць... чого же...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Luk'anenko_last_pages-0024_23.png</td>\n",
       "      <td>бону стилістики і навіть зміста</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_name  \\\n",
       "0               Ukrainka-0126_04.png   \n",
       "1  Luk'anenko_last_pages-0042_26.png   \n",
       "2               Ukrainka-0080_16.png   \n",
       "3            shchodennyk-0026_08.png   \n",
       "4  Luk'anenko_last_pages-0024_23.png   \n",
       "\n",
       "                                                text  \n",
       "0                  на одну підводу скрині та перини,  \n",
       "1                    загубив мене, відбив від життя.  \n",
       "2                -В Іванихи породіллі дитя калихала.  \n",
       "3  -Все розповів до найметшія дрібниць... чого же...  \n",
       "4                    бону стилістики і навіть зміста  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(image_dir, \"14484847_train.txt\"), index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(os.path.join(image_dir, \"14484847_test.txt\"), index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split cropped images in different directories\n",
    "\n",
    "test_images = f\"{image_dir}14484847_test/\"\n",
    "train_images = f\"{image_dir}14484847_train/\"\n",
    "os.makedirs(test_images, exist_ok=True)\n",
    "os.makedirs(train_images, exist_ok=True)\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    os.rename(os.path.join(image_dir, \"cropped_14484847/\") + row['file_name'], test_images + row['file_name'])\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    os.rename(os.path.join(image_dir, \"cropped_14484847/\") + row['file_name'], train_images + row['file_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
