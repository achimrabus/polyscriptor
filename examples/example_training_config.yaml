# Example HTR training config (sanitized for public use)
# Copy this file and replace placeholders with your own dataset/model settings.

# Core model selection
model_name: "<model-id>"            # e.g., "microsoft/trocr-base-handwritten" or your HF model ID
max_length: 128

# Data paths (use environment variables or relative paths)
data_root: "${DATA_ROOT:-./data}"   # defaults to ./data if DATA_ROOT is not set
train_csv: "${TRAIN_CSV:-train.csv}"
val_csv: "${VAL_CSV:-val.csv}"

# Output
output_dir: "${OUTPUT_DIR:-./models/example}"  # training artifacts

# Dataloader settings
cache_images: false
num_workers: 4

# Training hyperparameters
batch_size: 8
gradient_accumulation_steps: 1
epochs: 10
learning_rate: 0.0001
weight_decay: 0.01
warmup_ratio: 0.1
optim: "adamw"

# Optimization
fp16: true
gradient_checkpointing: false

# Evaluation and checkpointing
eval_strategy: "steps"
eval_steps: 500
save_steps: 500
save_total_limit: 2
logging_steps: 50

# Generation settings
predict_with_generate: true

# Reproducibility
seed: 42
